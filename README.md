# Vietnamese CTC Whisper ASR

Há»‡ thá»‘ng nháº­n dáº¡ng giá»ng nÃ³i tiáº¿ng Viá»‡t hiá»‡u suáº¥t cao dá»±a trÃªn CTC (Connectionist Temporal Classification) vá»›i PhoWhisper encoder.

## Tá»•ng Quan Dá»± Ãn

Model nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ nháº­n dáº¡ng giá»ng nÃ³i tiáº¿ng Viá»‡t vá»›i hiá»‡u suáº¥t vÆ°á»£t trá»™i so vá»›i Whisper gá»‘c cá»§a OpenAI. Sá»­ dá»¥ng kiáº¿n trÃºc CTC thay vÃ¬ autoregressive decoder, model Ä‘áº¡t Ä‘Æ°á»£c tá»‘c Ä‘á»™ inference nhanh hÆ¡n 3-5 láº§n vÃ  Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n cho tiáº¿ng Viá»‡t.

## âš¡ Nhá»¯ng KhÃ¡c Biá»‡t ChÃ­nh So Vá»›i Whisper Gá»‘c

### ğŸ—ï¸ Kiáº¿n TrÃºc Model

| ThÃ nh Pháº§n | Whisper Gá»‘c | Model NÃ y | Lá»£i Ãch |
|------------|-------------|-----------|---------|
| **Decoder** | Autoregressive Transformer | CTC Decoder | âš¡ Nhanh hÆ¡n 3-5x |
| **Memory** | Cao (decoder cache) | Tháº¥p (khÃ´ng cache) | ğŸ’¾ Tiáº¿t kiá»‡m 40% memory |
| **Stability** | Error propagation | Independent prediction | ğŸ¯ á»”n Ä‘á»‹nh hÆ¡n |

### ğŸ§  Encoder Cáº£i Tiáº¿n

- **PhoWhisper Encoder**: ÄÆ°á»£c pre-train Ä‘áº·c biá»‡t cho tiáº¿ng Viá»‡t
- **ALiBi Attention**: Xá»­ lÃ½ audio dÃ i tá»‘t hÆ¡n, há»— trá»£ streaming
- **Optimized Context**: KhÃ´ng bá»‹ giá»›i háº¡n 30 giÃ¢y nhÆ° Whisper gá»‘c

### ğŸ¯ CTC Decoder ThÃ´ng Minh

- **Prefix Beam Search**: ChÃ­nh xÃ¡c hÆ¡n standard beam search
- **Label Smoothing**: Giáº£m overfitting, cáº£i thiá»‡n generalization
- **Length Normalization**: CÃ¢n báº±ng giá»¯a Ä‘á»™ dÃ i vÃ  cháº¥t lÆ°á»£ng

### ğŸ“ Tokenizer ChuyÃªn Biá»‡t

- **SentencePiece BPE**: 1024 vocab size tá»‘i Æ°u cho tiáº¿ng Viá»‡t
- **Subword Handling**: Xá»­ lÃ½ tá»« ghÃ©p vÃ  tá»« má»›i tiáº¿ng Viá»‡t hiá»‡u quáº£
- **Compact Vocabulary**: Nhá» gá»n nhÆ°ng hiá»‡u quáº£ hÆ¡n tokenizer Ä‘a ngÃ´n ngá»¯

### ğŸµ Xá»­ LÃ½ Audio ThÃ´ng Minh

- **Flexible Length**: KhÃ´ng giá»›i háº¡n Ä‘á»™ dÃ i audio
- **Real-world Noise**: Inject tiáº¿ng á»“n thá»±c táº¿

## ğŸ“Š Chuáº©n Bá»‹ Dá»¯ Liá»‡u

### ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```text

whisper_asr/
â”œâ”€â”€ metadata.csv              # File metadata chÃ­nh
â”œâ”€â”€ datatest/                 # ThÆ° má»¥c chá»©a file audio
â”‚   â”œâ”€â”€ audio1.wav
â”‚   â”œâ”€â”€ audio2.wav
â”‚   â””â”€â”€ ...
â””â”€â”€ weights/                  # Model weights vÃ  tokenizer
    â”œâ”€â”€ phowhisper_small_encoder.pt
    â””â”€â”€ tokenizer_spe_bpe_v1024_pad/
        â””â”€â”€ tokenizer.model
```

### ğŸ“ Format Dá»¯ Liá»‡u

Model sá»­ dá»¥ng **má»™t file CSV duy nháº¥t** vá»›i format Ä‘Æ¡n giáº£n:

```csv
path|text
./datatest/audio1.wav|chá»¥p cá»™ng hÆ°á»Ÿng tá»« phÃ¡t hiá»‡n u tuyáº¿n yÃªn kÃ­ch thÆ°á»›c 8mm.
./datatest/audio2.wav|khi máº·t trá»i lÃ³ ráº¡ng, sÆ°Æ¡ng Ä‘á»ng trÃªn lÃ¡ bá»—ng long lanh hÆ¡n.
./datatest/audio3.wav|bá»‡nh nhÃ¢n nháº­p viá»‡n vá»›i cháº©n Ä‘oÃ¡n viÃªm cÆ¡ tim cáº¥p do virus.
```

### ğŸ”§ TÃ­nh NÄƒng Dá»¯ Liá»‡u

- **Auto Train/Val Split**: Tá»± Ä‘á»™ng chia 95% train, 5% validation
- **Text-only Filtering**: Lá»c theo Ä‘á»™ dÃ i text (1-60 kÃ½ tá»±)
- **No Duration Limit**: KhÃ´ng giá»›i háº¡n Ä‘á»™ dÃ i audio
- **Reproducible Split**: Sá»­ dá»¥ng random seed Ä‘á»ƒ Ä‘áº£m báº£o káº¿t quáº£ nháº¥t quÃ¡n

### ğŸ“‹ YÃªu Cáº§u Dá»¯ Liá»‡u

- **Audio Format**: WAV, MP3, FLAC (khuyáº¿n nghá»‹ WAV 16kHz)
- **Text Quality**: ChÃº Ã½ dáº¥u cÃ¢u vÃ  chÃ­nh táº£ tiáº¿ng Viá»‡t
- **File Size**: KhÃ´ng giá»›i háº¡n (model xá»­ lÃ½ Ä‘Æ°á»£c audio dÃ i)
- **Minimum**: Ãt nháº¥t 100 samples Ä‘á»ƒ test

## âš™ï¸ CÃ i Äáº·t

### ğŸ“¦ YÃªu Cáº§u Há»‡ Thá»‘ng

- Python 3.10+
- PyTorch 2.0+
- CUDA 11.8+ (khuyáº¿n nghá»‹)
- RAM: 8GB+ (16GB khuyáº¿n nghá»‹)
- GPU: 6GB+ VRAM

### ğŸ› ï¸ CÃ i Äáº·t Dependencies

```bash
# Clone repository
git clone https://github.com/iamdinhthuan/vietnamese-ctc-whispe
cd vietnamese-ctc-whisper

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# CÃ i Ä‘áº·t PyTorch (cho CUDA 11.8)
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118
```

### ğŸ“¥ Download Model Weights

```bash
wget https://huggingface.co/dinhthuan/phowhisper_small_encoder/resolve/main/phowhisper_small_encoder.pt -O weights/phowhisper_small_encoder.pt
```

## ğŸš€ Sá»­ Dá»¥ng

### 1ï¸âƒ£ Training CÆ¡ Báº£n

```bash
# Train vá»›i config máº·c Ä‘á»‹nh
python run.py

# Train vá»›i config tÃ¹y chá»‰nh
python run.py --config config.json

# Train vá»›i parameters override
python run.py --batch-size 16 --learning-rate 2e-4 --max-epochs 30
```

### 2ï¸âƒ£ Config TÃ¹y Chá»‰nh

```python
from config import get_config

# Load config máº·c Ä‘á»‹nh
config = get_config()

# TÃ¹y chá»‰nh parameters
config.training.batch_size = 32
config.training.learning_rate = 1e-4
config.model.dropout = 0.15

# LÆ°u config
config.save("my_config.json")

# Train vá»›i config má»›i
# python run.py --config my_config.json
```

### 3ï¸âƒ£ Inference

```bash
# Inference cÆ¡ báº£n
python inference.py --checkpoint checkpoints/best.ckpt --audio audio.wav

# Vá»›i beam search
python inference.py --checkpoint checkpoints/best.ckpt --audio audio.wav --beam_search

# Batch inference
python inference.py --checkpoint checkpoints/best.ckpt --audio_dir audio_folder/ --output results.txt
```

## âš™ï¸ Cáº¥u HÃ¬nh Chi Tiáº¿t

### ğŸµ Audio Processing

```json
{
  "audio": {
    "sample_rate": 16000,     // Táº§n sá»‘ láº¥y máº«u
    "n_mels": 80,            // Sá»‘ mel filters
    "n_fft": 400,            // FFT window size
    "hop_length": 160        // Hop length cho STFT
  }
}
```

### ğŸ§  Model Architecture

```json
{
  "model": {
    "n_state": 768,          // Dimension cá»§a model
    "n_head": 12,            // Sá»‘ attention heads
    "n_layer": 12,           // Sá»‘ transformer layers
    "vocab_size": 1024,      // KÃ­ch thÆ°á»›c vocabulary
    "dropout": 0.1,          // Dropout rate
    "label_smoothing": 0.1   // Label smoothing factor
  }
}
```

### ğŸ‹ï¸ Training Parameters

```json
{
  "training": {
    "batch_size": 16,              // Batch size
    "learning_rate": 1e-4,         // Learning rate ban Ä‘áº§u
    "max_epochs": 50,              // Sá»‘ epochs tá»‘i Ä‘a
    "precision": "bf16-mixed",     // Mixed precision training
    "gradient_clip_val": 1.0,      // Gradient clipping
    "num_sanity_val_steps": 0      // Táº¯t sanity checking
  }
}
```

### ğŸ“Š Data Configuration

```json
{
  "data": {
    "metadata_file": "metadata.csv",     // File metadata
    "train_val_split": 0.95,             // Tá»· lá»‡ train/val
    "min_text_len": 1,                   // Äá»™ dÃ i text tá»‘i thiá»ƒu
    "max_text_len": 60,                  // Äá»™ dÃ i text tá»‘i Ä‘a
    "enable_augmentation": true,         // Báº­t data augmentation
    "augmentation_prob": 0.8,            // XÃ¡c suáº¥t augmentation
    "random_seed": 42                    // Random seed
  }
}
```

## ğŸ“ˆ Tips Training Hiá»‡u Quáº£

### ğŸš€ Cho ThÃ­ Nghiá»‡m Nhanh

```python
config.model.n_state = 512        # Model nhá» hÆ¡n
config.model.n_layer = 6          # Ãt layers hÆ¡n
config.training.batch_size = 8     # Batch nhá»
config.training.max_epochs = 10    # Training ngáº¯n
```

### ğŸ¯ Cho Cháº¥t LÆ°á»£ng Cao

```python
config.model.n_state = 768        # Model Ä‘áº§y Ä‘á»§
config.training.batch_size = 32    # Batch lá»›n
config.training.max_epochs = 100   # Training dÃ i
config.data.augmentation_prob = 0.9  # Augmentation nhiá»u
```

### ğŸ­ Cho Production

```python
config.training.precision = "bf16-mixed"     // Mixed precision
config.training.accumulate_grad_batches = 2  // Gradient accumulation
config.training.num_workers = 8              // Parallel data loading
```

## ğŸ“Š Monitoring Training

### ğŸ“ˆ TensorBoard

```bash
# Khá»Ÿi Ä‘á»™ng TensorBoard
tensorboard --logdir checkpoints

# Xem táº¡i http://localhost:6006
```

### ğŸ” Metrics Quan Trá»ng

- **Training Loss**: Giáº£m Ä‘á»u theo epochs
- **Validation WER**: Word Error Rate (cÃ ng tháº¥p cÃ ng tá»‘t)
- **Learning Rate**: Theo OneCycle schedule
- **Step Time**: Thá»i gian má»—i step training

## ğŸ¯ Strategies Inference

### âš¡ Greedy Decoding

```python
# Nhanh nháº¥t, cháº¥t lÆ°á»£ng tá»‘t
inference = CTCInference(checkpoint_path, config)
result = inference.transcribe_single(audio_path, use_beam_search=False)
```

### ğŸ¯ Prefix Beam Search

```python
# Cháº­m hÆ¡n nhÆ°ng chÃ­nh xÃ¡c nháº¥t
result = inference.transcribe_single(
    audio_path, 
    use_beam_search=True,
    beam_size=10,
    length_penalty=0.3
)
```

## ğŸ› Troubleshooting

### âŒ Memory Issues

```python
# Giáº£m batch size
config.training.batch_size = 8

# Sá»­ dá»¥ng gradient accumulation
config.training.accumulate_grad_batches = 4

# Táº¯t cache audio
config.data.enable_caching = False
```

### ğŸŒ Training Cháº­m

```python
# Sá»­ dá»¥ng mixed precision
config.training.precision = "bf16-mixed"

# Táº¯t sanity checking
config.training.num_sanity_val_steps = 0

# Single-threaded data loading (Windows)
config.training.num_workers = 0
```

### ğŸ’¥ NaN Loss

```python
# Giáº£m learning rate
config.training.learning_rate = 5e-5

# TÄƒng gradient clipping
config.training.gradient_clip_val = 0.5

# Kiá»ƒm tra data quality
```

## ğŸ“š Examples Thá»±c Táº¿

### ğŸ™ï¸ Training vá»›i Medical Data

```python
# Config cho y táº¿
config = get_config()
config.data.metadata_file = "medical_data.csv"
config.model.dropout = 0.2  # TÄƒng dropout cho domain-specific
config.training.max_epochs = 200  # Training lÃ¢u hÆ¡n
config.save("medical_config.json")
```

### ğŸ“ Training vá»›i Call Center Data

```python
# Config cho call center
config.data.bg_noise_path = ["./noise/call_center/"]
config.data.augmentation_prob = 0.95  # Augmentation máº¡nh
config.audio.sample_rate = 8000  # Táº§n sá»‘ tháº¥p
config.save("callcenter_config.json")
```

## ğŸ¤ ÄÃ³ng GÃ³p

### ğŸ“ BÃ¡o Lá»—i

- Má»Ÿ issue trÃªn GitHub vá»›i thÃ´ng tin chi tiáº¿t
- Bao gá»“m config, logs vÃ  error message
- MÃ´ táº£ steps Ä‘á»ƒ reproduce

### ğŸ’¡ Äá» Xuáº¥t TÃ­nh NÄƒng

- MÃ´ táº£ rÃµ use case vÃ  lá»£i Ã­ch
- ÄÆ°a ra implementation approach náº¿u cÃ³ thá»ƒ
- Tháº£o luáº­n trong Discussions trÆ°á»›c khi code

### ğŸ”§ Pull Requests

- Fork repo vÃ  táº¡o feature branch
- Viáº¿t tests cho code má»›i
- Äáº£m báº£o code style consistency
- Update documentation náº¿u cáº§n

## ğŸ“„ License

MIT License - Xem file LICENSE Ä‘á»ƒ biáº¿t chi tiáº¿t.

## ğŸ“ LiÃªn Há»‡

- **GitHub Issues**: BÃ¡o bugs vÃ  feature requests
- **Email**: <bpyphuthien115@gmail.com>

## ğŸ™ Acknowledgments

- **PhoWhisper Team**: Cung cáº¥p pre-trained encoder cho tiáº¿ng Viá»‡t
- **OpenAI**: Whisper architecture inspiration
- **PyTorch Lightning**: Framework training máº¡nh máº½
- **SentencePiece**: Tokenization library hiá»‡u quáº£

---

â­ **Náº¿u project nÃ y há»¯u Ã­ch, hÃ£y star repo Ä‘á»ƒ á»§ng há»™ nhÃ©!** â­
